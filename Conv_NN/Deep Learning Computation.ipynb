{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c17416",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Layers and Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bc325",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now use code to generates a network with one fully-connected hidden layer with 256 units and ReLU activation, followed by a fully-connected output layer with 10 units.(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe43a66e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e892c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1090,  0.2617,  0.1134, -0.0333,  0.1857, -0.1067,  0.0774, -0.1264,\n",
       "         -0.0744,  0.1567],\n",
       "        [-0.0768,  0.1188,  0.1009,  0.0162,  0.0447, -0.0759,  0.0224, -0.1228,\n",
       "         -0.0717,  0.1000]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "            nn.Linear(20, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,10)\n",
    "                    )\n",
    "# initial a X-net\n",
    "X = torch.rand(2,20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769299f8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### A Custom Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bbde94",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the following snippet, we code up a block from scratch corresponding to an MLP with one hidden layer with 256 hidden units, and a 10-dimensional output layer.\n",
    "Note that the MLP class below inherits the class that represents a block. We will heavily rely on the parent class's functions, supplying only our own constructor and forward propagation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de3d5bfc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the MLP parent class 'Module' to perform the necessary initialization.\n",
    "        # In this way, other function arguments can also be specified during class instantiation, such as the model parameters\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f56767e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0905,  0.0689, -0.0605,  0.0758, -0.0441, -0.0462,  0.0725, -0.3150,\n",
       "          0.2012,  0.0401],\n",
       "        [-0.0036, -0.0122, -0.0459,  0.0017,  0.0017, -0.0534,  0.0480, -0.2251,\n",
       "          0.1995, -0.0778]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7704e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The Sequential Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ad320a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # When using the iterators, we need to keep track of the number of items in the iterator. \n",
    "            # This is achieved by an in-built method called enumerate(). \n",
    "            # The enumerate() method adds counter to the iterable.\n",
    "            # 'module' is an instance of a 'Module' subclass\n",
    "            self._modules[str(idx)] = module\n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27fccef5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0375,  0.1438, -0.2824,  0.0218, -0.2226,  0.0686, -0.0477,  0.1067,\n",
       "         -0.1525, -0.0817],\n",
       "        [ 0.0385,  0.1697, -0.2082,  0.0510, -0.1613,  0.0483,  0.0279,  0.0639,\n",
       "         -0.2172, -0.0457]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20,256), nn.ReLU(), nn.Linear(256,10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0515b08",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Executing Code in the Forward Propagation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3389952",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The Sequential class makes model construction easy, allowing us to assemble new architectures without having to define our own class. The following implement a FixedHiddenMLP class as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dabe046",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20,20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1) #torch.mm Matrix mutiple\n",
    "        # Reuse the fully-connected layer. This is equivalent to sharing parameters with two fully-connected layers\n",
    "        X = self.linear(X)\n",
    "        # Control Flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b9a321",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0194, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f53208",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " We can mix and match various ways of assembling blocks together. In the following example, we nest blocks in some creative ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f48ed9f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0877, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20,64), nn.ReLU(),\n",
    "                                 nn.Linear(64,32), nn.ReLU())\n",
    "        \n",
    "        self.linear = nn.Linear(32, 16)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "    \n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed85296",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc3fa5e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "~Layers are blocks.\n",
    "~Many layers can comprise a block.\n",
    "~Many blocks can comprise a block.\n",
    "~A block can contain code.\n",
    "~Blocks take care of lots of housekeeping, including parameter initialization and backpropagation.\n",
    "~Sequential concatenations of layers and blocks are handled by the Sequential block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5685c789",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}